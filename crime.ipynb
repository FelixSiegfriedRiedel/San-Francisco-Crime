{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **San Francisco Crime Analysis & Prediction**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Gliederung\n",
    "\n",
    "<!-- \n",
    "    Frame the problem and look at the big picture.\n",
    "    Get the data.\n",
    "    Explore the data to gain insights.\n",
    "    Prepare the data to better expose the underlying data patterns to Machine Learning algorithms.\n",
    "    Explore many different models and short-list the best ones.\n",
    "    Fine-tune your models and combine them into a great solution.\n",
    "    Present your solution.\n",
    "    Launch, monitor, and maintain your system.\n",
    "-->\n",
    "\n",
    "1. [Einleitung](#1-einleitung)\n",
    "2. [Aufgabenstellung](#2-problemstellung)\n",
    "3. [Import](#3-import)\n",
    "4. [Datenbereinigung](#4-datenbereinigung)\n",
    "5. [Exploration](#5-exploration)\n",
    "6. [Vorbereitung](#6-vorbereitung)\n",
    "7. [Modellierung](#7-modellierung)\n",
    "8. [Ergebnis](#8-ergebnis)\n",
    "\n",
    "\n",
    "\n",
    "# 1. Einleitung \n",
    "\n",
    "San Francisco war berüchtigt dafür, einige der weltweit bekanntesten Verbrecher auf der unentrinnbaren Insel Alcatraz unterzubringen. Heute ist die Stadt eher für ihre Technologieszene als für ihre kriminelle Vergangenheit bekannt. Ziel dieser Analyse ist eine Klassifizierung und Vorhersage von ausgewählten Verbrechenskategorien, basierend auf Zeit, Ort und weiteren Features. Als Grundlage hierfür dienen Kriminalberichte der letzten 14 Jahre, welche Daten aus allen Vierteln San Franciscos enthalten. \n",
    "\n",
    "\n",
    "# 2. Problemstellung\n",
    "\n",
    "<!-- Frame the problem and look at the big picture. -->\n",
    "\n",
    "Für die weitere Betrachtung werden im Folgenden die ersten Schritte unternommen, um eine Kategorie eines Verbrechens in San Francisco vorherzusagen. Um den Umfang der Daten einzuschränken beschränkt sich diese Analyse ausschließlich auf die nachstehenden Kategorien:\n",
    "\n",
    "\n",
    " *Larceny/Theft*, *Assault*, *Drug/Narcotic*, *Vehicle Theft* und *Burglary* \n",
    "\n",
    "\n",
    "Zur Vorhersage soll **eine** dieser Methoden verwendet werden: \n",
    "\n",
    "*Regression*, *Klassifikation* oder *Clustering*\n",
    "\n",
    "\n",
    "Im Verlauf der Analyse wird die Entscheidung zur Vorhersage einer Kategorie eines Verbrechens auf die Klassifikation fallen.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8601c4780bf9e11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Import\n",
    "\n",
    "Im ersten Abschnitt dieser Analyse werden verschiedene Python-Bibliotheken importiert, um Datenanalyse, Visualisierung und maschinelles Lernen durchzuführen. Außerdem werden die entsprechenden Daten importiert."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e92bc34f694c88e0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette(None, 5)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "import geoplot as gplt\n",
    "from geopandas import GeoDataFrame\n",
    "from geopandas.tools import sjoin\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "from meteostat import Point, Hourly\n",
    "\n",
    "from itertools import product\n",
    "from scipy import stats\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e77d15af49d6536b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Datenbereinigung \n",
    "\n",
    "Im nächsten Schritt folgt die Datenbereinigung, damit die Qualität der Daten überprüft werden kann. Außerdem werden Maßnahmen zur Bereinigung des Datensatzes durchgeführt."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc8b61f3b342bd9a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime = pd.read_csv(\"data/train.csv\")\n",
    "crime[\"CrimeId\"] = crime.index\n",
    "crime.head()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d480400e97ca04ae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zunächst werden die ersten fünf Zeilen aus dem Datensatz ausgegeben, um ein erstes Gefühl für die Daten und Kriminalitätsinformationen zu bekommen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "464c0ddcba5f51a1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime['Dates'] = pd.to_datetime(crime['Dates'])\n",
    "\n",
    "print('First date: ', str(crime['Dates'].min()))\n",
    "print('Last date: ', str(crime['Dates'].max()))\n",
    "print('crime data shape: ', crime.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2da4fd444fef8d6"
  },
  {
   "cell_type": "raw",
   "source": [
    "Hier wird sichergestellt, dass die 'Date'-Spalte im Datetime-Format vorliegt. Außerdem erlangen wir Informationen über das früheste und späteste Datum im Datensatz sowie die Form des DataFrames:\n",
    "\n",
    "Die Daten reichen vom 1. Juni 2003 bis zum 13. Mai 2015 und enthalten neun Merkmale innerhalb von 878.049 Datenpunkten.\n",
    "\n",
    "Folgende Variablen sind enthalten:\n",
    "\n",
    "Dates - Zeitstempel des Vorfalls\n",
    "Category - Kategorie des Vorfalls. (Dies ist unsere Zielvariable.)\n",
    "Descript - detaillierte Beschreibung des Vorfalls\n",
    "DayOfWeek - der Wochentag\n",
    "PdDistrict - der Name des Polizeirevierbezirks\n",
    "Resolution - Die Lösung des Vorfalls\n",
    "Address - die ungefähre Straßenadresse des Vorfalls\n",
    "X - Längengrad\n",
    "Y - Breitengrad"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49bf4bdc76e6ce9c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Selektion\n",
    "\n",
    "Wie in der Problemstellung beschrieben, werden die Daten so gefiltert, dass nur *Larceny/Theft*, *Assault*, *Drug/Narcotic*, *Vehicle Theft* und *Burglary* als Kategorien vertreten sind."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9ae0cc2edee8253"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime[\"Category\"].unique()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a019235b8bd54eb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "categories = [i.upper() for i in [\"Larceny/Theft\", \"Assault\", \"Drug/Narcotic\", \"Vehicle Theft\", \"Burglary\"]]\n",
    "\n",
    "crime = crime.loc[crime[\"Category\"].isin(categories)]\n",
    "\n",
    "crime.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ff9e1eec8ea7f0b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "82a57c61152c1b15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Duplikate\n",
    "\n",
    "Im nächsten Schritt werden die Duplikate im Datensatz untersucht:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b55f07213e53d537"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime.duplicated().sum()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73fe4fe7633726c9"
  },
  {
   "cell_type": "raw",
   "source": [
    "Der Datensatz enthält nach Filterung der entsprechenden Kategorien noch 923 Duplikate, welche im nächsten Schritt entfernt werden. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5888c9d821e07b82"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime.drop_duplicates(inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31d0e847c41cea82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datentypen"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a7327008ed9c9fe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime.dtypes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "122d576c0a5bb261"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ein Blick auf die Datentypen zeigt, dass sie bereits in einem gänfigen Format vorliegen.\n",
    "Lediglich die 'Dates' Spalte wird weiter unterteilt für spätere Analysen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b634f94f426f3936"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime[\"Dates\"] = pd.to_datetime(crime[\"Dates\"])\n",
    "crime[\"YearMonth\"] = crime['Dates'].dt.strftime('%Y-%m')\n",
    "crime[\"YearMonthDay\"] = crime['Dates'].dt.strftime('%Y-%m-%d')\n",
    "crime[\"MonthDay\"] = crime['Dates'].dt.strftime('%m-%d')\n",
    "crime[\"Year\"] = crime['Dates'].dt.strftime('%Y')\n",
    "crime[\"Month\"] = crime['Dates'].dt.strftime('%m')\n",
    "crime[\"Day\"] = crime['Dates'].dt.strftime('%d')\n",
    "crime[\"TimeOfDay\"] = crime['Dates'].dt.strftime('%H')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f9e50afe78cb11b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Falsche Werte\n",
    "\n",
    "Im nächsten Schritt wird betrachtet, ob die Koordinaten falsche Werte enthalten könnten."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6410560c62c1d5c5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "import seaborn as sns\n",
    "\n",
    "def create_gdf(df):\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.X, df.Y), crs='epsg:4326')\n",
    "    return gdf\n",
    "\n",
    "\n",
    "crime_gdf = create_gdf(crime)\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "world.plot(ax=ax, color='white', edgecolor='black')\n",
    "crime_gdf.plot(ax=ax, color='red', markersize=10)  # Adjust markersize as needed\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "#sns.despine(ax=ax, left=True, right=True, top=True, bottom=True)\n",
    "\n",
    "ax.set_xticks([])  # Use set_xticks to hide x-axis ticks\n",
    "ax.set_yticks([])  # Use set_yticks to hide y-axis ticks\n",
    "\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20d1b7dba8539f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ein Blick auf die Karte genügt um zu sehen, dass sich einige Punkte außerhalb der USA und außerhalb von San Fransisco befinden. Diese Punkte werden konkret lokalisiert und im nächsten Schritt ausgegeben. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49128cd5d70f8be0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(crime_gdf.loc[crime_gdf.Y > 50].count()[0])\n",
    "crime_gdf.loc[crime_gdf.Y > 50].sample(5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e25631d604f130f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zusammengefasst gibt der Code also die Anzahl der Datensätze mit einer Y-Koordinate größer als 50 aus und zeigt dann fünf zufällige Datensätze mit dieser Bedingung an. Dies könnte darauf hindeuten, dass es im GeoDataFrame einige Datensätze mit ungewöhnlichen oder fehlerhaften geografischen Koordinaten gibt. Damit diese auffälligen Daten nicht aus der Analyse ausgeschlossen werden, können die Mittelwerte der vorhandenen Koordinaten der jeweiligen Polizeidistrikte genutzt werden."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fe83d72e82911fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n",
    "\n",
    "imp = SimpleImputer(strategy='mean')\n",
    "\n",
    "for district in crime['PdDistrict'].unique():\n",
    "    crime.loc[crime['PdDistrict'] == district, ['X', 'Y']] = imp.fit_transform(\n",
    "        crime.loc[crime['PdDistrict'] == district, ['X', 'Y']])\n",
    "\n",
    "crime_gdf = create_gdf(crime)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "445873e4f210cde1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Es wird folglich für jedes einzigartige Polizeidistrikt ('PdDistrict') im DataFrame crime der Imputer verwendet, um fehlende Werte in den Spalten 'X' und 'Y' durch den Mittelwert der vorhandenen Werte im jeweiligen Distrikt zu ersetzen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0680937559c9344"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fehlende Werte"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a4e7287f7e84556"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5ee9b848d399bce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if sum(crime.isnull().any()*1):\n",
    "    print(\"Es gibt fehlende Daten.\")\n",
    "else:\n",
    "    print(\"Es gibt keine fehlenden Daten\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b6b5bfee655645d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nachdem die Koordinaten bereinigt wurden, werden weitere fehlende Werte berücksichtigt. Dafür wird die obenstehende Funktion verwendet um die Anzahl der fehlenden Werte in jedem Attribut (Spalte) des DataFrames zu zählen.\n",
    "\n",
    "Wie aus der Tabelle abgelesen werden kann, gibt es für alle Kategorien keine fehlenden Werte. \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf8e1a4fe59d067b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Exploration\n",
    "\n",
    "Nachdem die Datenbereinigung abgeschlossen wurde, wird nun der Fokus auf die Exploration der Daten gelegt. Hierbei handelt es sich um einen Prozess, bei dem der Datensatz analysiert wird, um ein besseres Verständnis für die enthaltenen Variablen zu entwickeln. Ziel ist es, Muster, Trends oder ungewöhnliche Beobachtungen zu identifizieren. Dieser Prozess hilft bei der Vorbereitung der Daten für die weitere Analyse und Modellbildung.\n",
    "\n",
    "## Deskriptive Statistik"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4998c1f1628a5db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Balkendiagramme\n",
    "\n",
    "#### Verbrechen je Bezirk"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "596790a60244217"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_cat = crime.groupby('Category').count().iloc[:, 0]. sort_values(ascending=False)\n",
    "data = data_cat.reindex(np.append(np.delete(data_cat.index, 1), 'OTHER OFFENSES'))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e307e2a2d08893c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    ax = sns.barplot(\n",
    "        x = (data_cat.values / data_cat.values.sum()) * 100,\n",
    "        y = data_cat.index,\n",
    "        orient='h',\n",
    "        palette=\"Blues_r\")\n",
    "\n",
    "plt.title('Incidents per Crime Category', fontdict={'fontsize': 16})\n",
    "plt.xlabel('Incidents (%)')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b1db039132e5e16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Für einen ersten Überblick werden Einzelfälle pro Kategorie in Prozent aufgeschlüsselt. Es ist deutlich zu erkennen, dass 'Larceny/Theft' nahezu 50 Prozent der Fälle ausmacht. Dahingegen werden nur knapp unter 10 Prozent der Fälle als 'Burglary' klassifiziert.'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51486d931632b59b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_cat.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e64d65a4593ecb71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Für die weitere Einordnung wird die Funktion describe() genutzt. Wie bereits in der Selektion deutlich wurde, wird in der Analyse nur mit den fünf ausgewählten Kategorien gearbeitet. Außerdem können einige grundlegende statistische Maße abgelesen werden."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0cc5f494ee14207"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Histogramme"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1994253b4e23722"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def truncate_label(label, length=5):\n",
    "    return label[:length]\n",
    "\n",
    "colors = sns.color_palette(None, 3)\n",
    "columns = [\"DayOfWeek\", \"PdDistrict\", \"Resolution\"]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize = (10, 15), tight_layout=True)\n",
    "\n",
    "axes[0, 0].hist(crime[\"Category\"], bins=5, align=\"mid\")\n",
    "\n",
    "#Tag der Woche als Zahl für Sortierung der Balken\n",
    "axes[0, 1].hist(crime['Dates'].dt.weekday + 1, bins=7, align=\"mid\")\n",
    "\n",
    "axes[1, 0].hist(crime[\"PdDistrict\"], bins=len(crime[\"PdDistrict\"].unique()))\n",
    "axes[1, 1].hist(crime[\"Resolution\"], bins=len(crime[\"Resolution\"].unique()))\n",
    "axes[2, 0].hist(crime[\"Year\"].sort_values(), bins=13)\n",
    "axes[2, 1].hist(crime[\"Month\"].sort_values(), bins=12)\n",
    "axes[3, 0].hist(crime[\"X\"], bins=50)\n",
    "axes[3, 1].hist(crime[\"Y\"], bins=50)\n",
    "\n",
    "# Rotate x-axis tick labels\n",
    "for ax in axes.flatten():\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.xaxis.set_ticks_position('bottom')  # Place ticks at the bottom for better visibility\n",
    "    ax.set_xticklabels([truncate_label(label.get_text()) for label in ax.get_xticklabels()])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9f78f553f0c1bcf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hier werden Histogramme für verschiedene Merkmale des Kriminalitätsdatensatzes erstellt, einschließlich der Kategorie der Straftaten, dem Wochentag, dem Polizeibezirk, der Auflösung, dem Jahr, dem Monat sowie den geografischen Koordinaten X und Y. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecfd779c9ebca743"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Zeitliche Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8885e9e230cbb25"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "sns.set_context('notebook', font_scale = 1)\n",
    "\n",
    "crime_year_cat = crime.groupby(by=[\"Year\", \"Category\"], as_index=False).count()\n",
    "data = crime_year_cat.pivot_table(index=\"Year\", columns=\"Category\", values=\"Descript\")\n",
    "\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "\n",
    "for category in categories:\n",
    "    plt.plot(data[category], label=category)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "tix = plt.xticks()[0]\n",
    "plt.xticks(tix, rotation=90, ha=\"center\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dbeb10e1e6c2df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Es folgt eine Visualising der zeitlichen Regression und Entwicklung der Straftaten in verschiedenen Kategorien über die Jahre gruppiert nach 'Year', wodurch Trends und Muster im zeitlichen Verlauf sichtbar werden. Besonders Auffällig ist der Abfall von 'Larceny/Theft' nach dem Jahr 2014. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dd05309712b8875"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime_yearmonth_cat = crime.groupby(by=[\"YearMonth\", \"Category\"], as_index=False).count()\n",
    "data = crime_yearmonth_cat.pivot_table(index=\"YearMonth\", columns=\"Category\", values=\"Descript\")\n",
    "\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "\n",
    "for category in categories:\n",
    "    plt.plot(data[category], label=category)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "tix = plt.xticks()[0]\n",
    "plt.xticks(tix[::6], rotation=90, ha=\"center\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb7e6ea5763c7c12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Diese Visualisierung zeigt ebenfalls die zeitliche Entwicklung der Straftaten nach den jeweiligen Kategorien. In diesem Fall wird jedoch eine Gruppierung nach 'YearMonth' vorgenommen, damit eine feinere zeitliche Auflösung ersichtlich wird. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "305e03f6771a0420"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Kartogramme"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dd884e048ea3dd6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "sf_df = gpd.read_file(\"data/SF Find Neighborhoods.geojson\").to_crs({'init': 'epsg:4326'})\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(crime.X, crime.Y)]\n",
    "crime_gdf = GeoDataFrame(crime, crs=\"EPSG:4326\", geometry=geometry)\n",
    "\n",
    "\n",
    "point = crime_gdf\n",
    "poly  = sf_df\n",
    "\n",
    "pointInPolys = sjoin(point, poly, how='left')\n",
    "pointInPolys.drop_duplicates(subset=['Dates', 'Category', 'Descript', 'DayOfWeek', 'PdDistrict',\n",
    "                                     'Resolution', 'Address', 'X', 'Y', 'YearMonth', 'Year', 'Month', 'Day',\n",
    "                                     'TimeOfDay'], inplace=True)\n",
    "\n",
    "pointInPolys = pointInPolys.rename(columns ={\"name\" : \"crdistrict\"})\n",
    "\n",
    "crime = crime.join(pointInPolys[[\"CrimeId\", \"crdistrict\"]], on=\"CrimeId\", lsuffix=\"_\")\n",
    "\n",
    "grouped = pointInPolys.groupby('index_right').count()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b87c10d1ec40726"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hier wird eine GeoDataFrame (sf_df) aus einer GeoJSON-Datei eingelesen, die die Grenzen der Stadtviertel von San Francisco enthält. Die to_crs-Methode wird verwendet, um die Koordinatenreferenz des GeoDataFrames auf das Standardformat (EPSG:4326) zu ändern, das Längen- und Breitengrade verwendet. \n",
    "Vgl. City and County of San Francisco. (2016). SF Find Neighborhoods. [Dataset]. Socrata. https://data.sfgov.org/Geographic-Locations-and-Boundaries/SF-Find-Neighborhoods/pty2-tcw4\n",
    "\n",
    "Insgesamt ermöglicht dieses Vorgehen die Zuordnung von Kriminalitätsdaten zu den entsprechenden Stadtvierteln von San Francisco und die Berechnung der Anzahl der Vorkommen in jedem Viertel. Dies ist nützlich, um räumliche Muster und Hotspots von Kriminalität zu identifizieren."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8db62476957bdeab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "heat_districts = pointInPolys.groupby(['index_right', 'crdistrict'])[\"Dates\"].count()\n",
    "heat_districts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5fd5a88a10621bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In heat_districts werden dafür Indexwerte für die einzelnen Stadtviertel erstellt. Die Werte repräsentieren dabei die Anzahl der Kriminalitätsvorfälle in jedem Stadtviertel."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60ea1cbf6919a153"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "geom = sf_df.pop('geometry')\n",
    "sf_df = sf_df.join(geom, how='inner')\n",
    "sf_df[\"polygons\"] = \"\"\n",
    "for i,row in sf_df.iterrows():\n",
    "    geometry = row[\"geometry\"]\n",
    "    if geometry.geom_type == 'MultiPolygon':\n",
    "        polygons = []\n",
    "        for polygon in geometry.geoms:\n",
    "            exterior_coords = list(polygon.exterior.coords)\n",
    "            interior_coords = [list(interior.coords) for interior in polygon.interiors]\n",
    "            polygons.append(Polygon(exterior_coords, interior_coords))\n",
    "    else:\n",
    "        polygons = [Polygon(list(geometry.exterior.coords))]\n",
    "    sf_df.at[i, \"polygons\"] = polygons\n",
    "\n",
    "sf_df[\"polygons\"] = sf_df[\"polygons\"].explode()\n",
    "\n",
    "sf_df[\"index_right\"] = sf_df.index\n",
    "sf_df[\"heat\"] = sf_df.merge(heat_districts, on=\"index_right\")[\"Dates\"]\n",
    "sf_df[\"geometry\"] = sf_df[\"polygons\"]\n",
    "\n",
    "gdf = gpd.GeoDataFrame(sf_df)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65846bf193196a3d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(10, 20), layout='constrained')\n",
    "\n",
    "gdf.plot(ax=ax[0], alpha=1, edgecolor='k', linewidth=0.5, column=\"heat\", cmap=\"PuRd\")\n",
    "cbar = plt.colorbar(ax[0].get_children()[0], ax=ax[0], orientation=\"horizontal\", shrink=0.6)\n",
    "cbar.set_label(\"Total Amount of Crimes\")\n",
    "\n",
    "sf_df = gpd.read_file(\"data/SF Find Neighborhoods.geojson\").to_crs({'init': 'epsg:4326'})\n",
    "sf_df.plot(ax=ax[1], alpha=0.2, edgecolor='k', linewidth=0.5, zorder=2)\n",
    "ax[1].scatter(data=crime.loc[crime[\"Y\"]<80], x=\"X\", y=\"Y\",alpha=0.2, color=\"pink\", zorder=1)\n",
    "\n",
    "ax[1].set_xlim(ax[0].get_xlim())\n",
    "ax[1].set_ylim(ax[0].get_ylim())\n",
    "ax[0].set_aspect('equal', adjustable='box')\n",
    "ax[1].set_aspect('equal', adjustable='box')\n",
    "\n",
    "sns.despine(ax=ax[0], left=True, right=True, top=True, bottom=True)\n",
    "sns.despine(ax=ax[1], left=True, right=True, top=True, bottom=True)\n",
    "\n",
    "ax[0].xaxis.set_ticks([])\n",
    "ax[0].yaxis.set_ticks([])\n",
    "\n",
    "ax[1].xaxis.set_ticks([])\n",
    "ax[1].yaxis.set_ticks([])    \n",
    "    \n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9dcee32c68b472d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Insgesamt kann in der Heat-Map ein deutliches Muster erkannt werden. Die Kriminalfälle bündeln sich besonders in den Stadtvierteln im Nord-Osten von San Francisco."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50a7d83ecef34481"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Engineering\n",
    "\n",
    "An dieser Stelle werden neue Spalten generiert, die den Datensatz bereichern und den Modellen helfen können."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "439d318b3f183d48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feiertage\n",
    "\n",
    "Zuerst werden die US-Feiertage importiert und dem Datensatz hinzugefügt."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae1eb668a2196143"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "usa_holidays = pd.Series(holidays.country_holidays('US',  years=range(crime[\"Dates\"].dt.year.min(), crime[\"Dates\"].dt.year.max())))\n",
    "\n",
    "usa_holidays = pd.DataFrame(usa_holidays)\n",
    "usa_holidays.columns = [\"Holiday\"]\n",
    "usa_holidays[\"YearMonthDay\"] = pd.to_datetime(usa_holidays.index)\n",
    "usa_holidays.reset_index(drop=True)\n",
    "\n",
    "crime[\"YearMonthDay\"] = pd.to_datetime(crime[\"YearMonthDay\"])\n",
    "crime = crime.merge(usa_holidays, on=\"YearMonthDay\", how=\"left\")\n",
    "\n",
    "crime[\"Holiday\"].fillna(\"None\", inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "401cfd5b617b222c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d324c988e13f201a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_district_holiday = pd.crosstab(crime['Category'], crime['Holiday'])\n",
    "df_district_holiday = df_district_holiday.loc[:, df_district_holiday.columns != 'None']\n",
    "df_district_holiday.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ca9836e7c27c746"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(df_district_holiday, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Category vs Holiday')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5695919b63cf53a4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In dieser Heat-Map werden die Feiertage den Kriminalkategorien entgegengestellt. Hier lässt sich ablesen, dass es die meisten Verbechen am Labor Day und Washingtons Birthday gibt."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a45ae732809315b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Wetter\n",
    "\n",
    "Die Daten werden um ein weiteres Feature erweitert. Heirfür werden die Wetterdaten für die entsprechende Region importiert."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbbf398db69fe816"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a297c502a6afaeac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime[\"hours\"] = pd.to_datetime(crime['Dates'].dt.strftime('%Y-%m-%d %H'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f1ab4dbb78fda5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from meteostat import Point, Hourly\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_date = crime['Dates'].min()\n",
    "end_date = crime['Dates'].max()\n",
    "\n",
    "san_francisco = Point(37.7749, -122.4194, 10)\n",
    "\n",
    "# Get hourly data for the specified date range\n",
    "sf_weather = pd.DataFrame(Hourly(san_francisco, start_date, end_date).fetch())\n",
    "\n",
    "sf_weather[\"hours\"] = pd.to_datetime(sf_weather.index.strftime('%Y-%m-%d %H'))\n",
    "crime = crime.merge(sf_weather, how=\"left\", on=\"hours\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0d3b3b5f7236fd5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85a8fcdae3f8e424"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "temp_cat_ct = pd.crosstab(crime[\"temp\"], crime[\"Category\"],  normalize='index')\n",
    "temp_cat_ct.plot(kind=\"area\",  stacked=\"true\", figsize=(12, 8))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3a1d1732e02a465"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Straßen\n",
    "\n",
    "Außerdem werden Straßen dem Datensatz hinzugefügt. Ziel ist es eine Übersicht der *gefährlichsten* Straßen in San Francisco zu bekommen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7acbb8209429bda9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "is_block = crime[\"Address\"].str.contains(\" /\")\n",
    "crime.loc[is_block, \"Street/Block\"] = crime.loc[is_block, \"Address\"].copy()\n",
    "crime.loc[is_block, \"Street_1\"] = crime.loc[is_block, \"Address\"].apply(lambda x: x.split(\" /\")[0]).copy()\n",
    "crime.loc[is_block, \"Street_2\"] = crime.loc[is_block, \"Address\"].apply(lambda x: x.split(\" /\")[1]).copy()\n",
    "\n",
    "is_street = crime[\"Address\"].str.contains(\" of \")\n",
    "crime.loc[is_street, \"Street/Block\"] = crime.loc[is_street, \"Address\"].apply(lambda x: x.split(\" of \")[1]).copy()\n",
    "crime.loc[is_street, \"Street_1\"] = crime.loc[is_street, \"Address\"].apply(lambda x: x.split(\" of \")[1]).copy()\n",
    "crime.loc[is_street, \"Street_2\"] = None\n",
    "crime.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13fae639781ec5fe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "str_cat_ct = pd.crosstab(crime[\"Street/Block\"], crime[\"Category\"], margins=True, margins_name=\"Total\")\n",
    "\n",
    "str_cat_ct = str_cat_ct[str_cat_ct.index != \"Total\"]\n",
    "top_ten_str = str_cat_ct.sort_values(by=\"Total\", ascending=False).head(10)\n",
    "top_ten_str"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "587c07040e14b419"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Anhand der Tabelle wird deutlich, dass bei den Top drei der gefährlichsten Straßen mit den meisten Kriminalfällen um die Market St, Mission St und Bryant St handelt. Hierbei ist jedoch die Länge der Straße zu beachten, damit diese Werte in ein geeignetes Verhältnis gesetzt werden können. Einige Straßen reichen durch nahezu die komplette Stadt, während andere kurz bemessen sind."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "676f19e4f6f0277c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zuletzt folgt die Visualisierung der Straßen anhand von einem Balkendiagramm."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e93a4c2e3697333"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "top_ten_str.drop('Total', axis=1).plot(kind='bar', stacked=True, figsize=(12, 8))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8776229b84668e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Korrelation\n",
    "\n",
    "Nachfolgend werden weitere Korrelationen berechent und visualisiert, um erneut Muster und Zusammenhänge in den Daten zu erkennen.\n",
    "\n",
    "#### Korrelation zwischen Kategorien und Bezirken\n",
    "\n",
    "Dafür wird im ersten Schritt eine Kreuztabelle für den Bezirk und die Kategorie erstellt."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba6d3d2d38c85bca"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_district_cat = pd.crosstab(crime['Category'], crime['PdDistrict'])\n",
    "df_district_cat.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ba22826b8765295"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(df_district_cat, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Category vs District')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e25e6395cecf4b8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Anhand dieser Heat-Map wird ersichtlich, dass es besonders im Bezitk 'Southern' ein hohes Aufkommen von 'Larceny/Theft'-Kriminalfällen gibt."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5efb15047293e15e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Korrelation zwischen den kategorischen Spalten\n",
    "\n",
    "Für die weitere Betrachtung wird die Korrelation zwischen den kategorischen Spalten betrachtet."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c31c49f0b64be6d9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "unique_val_col = pd.DataFrame(crime.nunique())\n",
    "corr_columns = list(unique_val_col.loc[(unique_val_col[0] > 0) & (unique_val_col[0] <= 2500)].index)\n",
    "print(corr_columns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10b850472d39f87"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "x = [i[0] for i in product(corr_columns, corr_columns)]\n",
    "y = [i[1] for i in product(corr_columns, corr_columns)]\n",
    "\n",
    "\n",
    "corr_df = pd.DataFrame(index=corr_columns, columns=corr_columns)\n",
    "\n",
    "for i in range(0,len(x)):\n",
    "    if x[i] == y[i]:\n",
    "        corr_df.loc[x[i], y[i]] = 1\n",
    "        corr_df.loc[y[i], x[i]] = 1\n",
    "    if pd.isnull(corr_df.loc[x[i], y[i]]):\n",
    "        temp_ct = pd.crosstab(crime[x[i]], crime[y[i]])\n",
    "\n",
    "        X2 = stats.chi2_contingency (temp_ct, correction= False )[0]\n",
    "        n = sum(temp_ct.sum())\n",
    "        minDim = min( temp_ct.shape )-1\n",
    "\n",
    "        V = np.sqrt((X2/n) / minDim)\n",
    "\n",
    "        corr_df.loc[x[i], y[i]] = V\n",
    "        corr_df.loc[y[i], x[i]] = corr_df.loc[x[i], y[i]]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d75f4538fcc20c38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hierfür wurde die Berechnung von Cramers-V genutzt.\n",
    "\n",
    "Vgl. Cramér, H. (1946). Mathematical Methods of Statistics. Princeton: Princeton University Press, p. 282 (Chapter 21. The two-dimensional case). ISBN 0-691-08004-6."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9142781da85287a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "corr_df = corr_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "mask = np.zeros_like(corr_df)\n",
    "mask[np.triu_indices_from(mask, k=1)] = True\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5, mask=mask)\n",
    "plt.title('Korrelations Matrix ')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "709daacf27959eda"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "__corr_df = corr_df.loc[corr_df[\"Category\"] >= 0.09]\n",
    "#__corr_df = __corr_df.drop([\"YearMonth\", \"Year\", \"Street_2\"])\n",
    "__corr_df = __corr_df.apply(pd.to_numeric, errors='coerce')\n",
    "__corr_df = __corr_df[__corr_df.index]\n",
    "\n",
    "mask = np.zeros_like(__corr_df)\n",
    "mask[np.triu_indices_from(mask, k=1)] = True\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(__corr_df, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5, mask=mask)\n",
    "plt.title('Korrelations Matrix ')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "136ae7832c7f9d4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Insgesamt ermöglicht diese Heatmap eine visuelle Darstellung der Korrelationsmatrix. Durch die Farben und die annotierten Werte können leicht Muster und Stärke der Korrelationen zwischen den verschiedenen Variablen im Datensatz erkannt werden."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85d6567b5317a5c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Vorbereitung"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55da83117d77733e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Anhand der Korrelation können wir ablesen, welche Spalten den größten Einfluss auf die Kategorie hat, welche wir letztendlich vorhersagen wollen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e49b222f2cc067c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "__corr_df[\"Category\"]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2154c74cc590d541"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Im nächsten Schritt werden die Spalten ausgewählt, welche mindestens eine Korrelation von 0.05 haben\n",
    "\n",
    "\n",
    "Da Year und YearMonth fast die gleiche Korrelation mit Category haben und untereinander eine sehr hohe Korrelation haben, \n",
    "beschränken wir uns auf YearMonth und filtern Year raus. \n",
    "\n",
    "<!-- Resolution ist leider nicht in den Test-Daten vorhanden, daher nützt es nichts die Modelle mit dieser Spalte zu trainieren und sie wird ebenfalls entfernt.  -->\n",
    "\n",
    "Und da wir Category vorhersagen sollen, wird es auch im Training nicht berücksichtigt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad266a82639053af"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "onehot_cols = __corr_df.index\n",
    "\n",
    "\n",
    "#Obwohl Resolution für die Kaggle-Challange nicht verwendet werden darf, darf es im Rahmen des Projekts zur Klassifikation dienen.\n",
    "#onehot_cols.remove(\"Resolution\")\n",
    "\n",
    "\n",
    "onehot_cols = onehot_cols.drop([\"Resolution\", \"Category\", \"Descript\", \"YearMonth\", 'YearMonthDay', \"Year\", \"Street_2\", \"Street_1\"])\n",
    "onehot_cols"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "623772b9a7fb1c01"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One-Hot-Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e02510755b7bcc42"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crime_oh = pd.get_dummies(data=crime, columns=onehot_cols, dtype=float, prefix=[\"_\" + i for i in onehot_cols])\n",
    "crime_oh = crime_oh.drop([\"CrimeId_\", \"Street_2\", \"Street_1\"], axis=1)\n",
    "crime_oh[\"Category\"] = crime[\"Category\"]\n",
    "crime_oh.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae7ce6f3ae92d7a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Daten Angleichen"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b068aeec1e3e635c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Die Kategorien sind momentan unausgeglichen, was letztendlich zu einem Bias im Modell führen kann. \n",
    "Hierfür wurde SMOTE in Kombination mit Undersampling verwendet, also die Daten werden zu erst auf basis der vorhandenen Daten hochskaliert und dann werden zufällige Werte gleichermaßen gelöscht. \n",
    "Diese Kombination funktioniert besser als reines Undersampling (Vgl. Chawla, N. V., Bowyer, K. W., Hall, L. O. & Kegelmeyer, W. P. (2002). SMOTE: Synthetic Minority Over-sampling technique. Journal Of Artificial Intelligence Research, 16, 321–357. https://doi.org/10.1613/jair.953)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8d3050a267d12af"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "\n",
    "X, y = crime_oh.filter(like='_', axis=1), crime_oh[\"Category\"]\n",
    "print(\"Shape vor Angleichung\",X.shape)\n",
    "class_size = round(X.shape[0] / 5 * 0.7)\n",
    "\n",
    "oversampling = SMOTE()\n",
    "undersampling = RandomUnderSampler(sampling_strategy={i: class_size for i in categories})\n",
    "# steps = [('SMOTE', oversampling), ('RandomUnderSampler', undersampling)]\n",
    "steps = [('SMOTE', oversampling)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "X, y = pipeline.fit_resample(X, y)\n",
    "print(\"Shape nach Angleichung\",X.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82bf43138c0cee42"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i in range(0,10,1)[0:20]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0fb4f56f3c20d4b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Modellierung"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2aa1f288540de4f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entscheidungsbaum"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad1b6ade816a56d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Daten Filtern"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266a6bc02551c4e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Um die Modelle und Rechenleistung zu verbessern, müssen die Spalten, die den Modellen übergeben werden (Feautures) reduziert werden.\n",
    "Mit Hilfe des ChiQuadrat Tests können die relevantesten Spalten gefunden werden."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3eab5d12c252883e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "feature_selector = SelectKBest(chi2, k=100).fit(X, y)\n",
    "X_new = X.iloc[:,feature_selector.get_support(indices=True)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8348777a17fedf51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kreuzvalidierung eines einfachen Entscheidungsbaumes mit den 100 besten Spalten"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a227d12b1cad8ee"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "cross_validate(DecisionTreeClassifier(), X, y, cv=5)[\"test_score\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53956b22336e51f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Kreuzvalidierung eines einfachen Entscheidungsbaumes mit allen Spalten"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87c3a6a69191f1a2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cross_validate(DecisionTreeClassifier(), X, y, cv=5)[\"test_score\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c54865d4a187dde"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Es wird deutlich, dass alle Feautures nicht zwangsweise das Modell verbessern, also gilt es herauszufinden, welche Anzahl die beste ist."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "799efe3891bdd1aa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# decision_tree = DecisionTreeClassifier(\n",
    "#     criterion         = 'entropy',\n",
    "#     max_depth         = 16,\n",
    "#     max_leaf_nodes    = 80,\n",
    "#     min_samples_leaf  = 1,\n",
    "#     min_samples_split = 500\n",
    "# )\n",
    "\n",
    "# decision_tree = DecisionTreeClassifier()\n",
    "# \n",
    "# scores = []\n",
    "# k=2\n",
    "# while k < X.shape[1]/2:\n",
    "#     feature_selector = SelectKBest(chi2, k=k).fit(X, y)\n",
    "#     X_temp = X.iloc[:,feature_selector.get_support(indices=True)]\n",
    "#     cv_results = cross_validate(decision_tree, X, y, cv=5)\n",
    "#     print(\"k\\t\\t=\\t\",k)\n",
    "#     print(\"score\\t=\\t\",cv_results['test_score'].mean())\n",
    "#     print(\"-------------------------------------------------\")\n",
    "#     scores = np.append(scores, (k, cv_results['test_score'].mean()))\n",
    "#     k=k*2\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "451e4aad9a2a7863"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "36fd7bd03b3185f3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame.from_records([(scores[i],scores[i+1]) for i in range(0,len(scores),2)],  columns=[\"k\", \"score\"])\n",
    "sns.lineplot(scores_df, x=\"k\", y=\"score\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d10c14897586852"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ein Entscheidungsbaum mit Standardeinstellungen lieferten die besten Werte bei einer Spaltenanzahl zwischen 32 und 128.\n",
    "\n",
    "Nun kann das Prozedere wiederholt werden mit kleineren Schritten."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f422188bda831d1b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "97bda0be35c74f10"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "scores = []\n",
    "for k in range(30,130,10):\n",
    "    feature_selector = SelectKBest(chi2, k=k).fit(X, y)\n",
    "    X_temp = X.iloc[:,feature_selector.get_support(indices=True)]\n",
    "    cv_results = cross_validate(decision_tree, X_temp, y, cv=5)\n",
    "    print(\"k\\t\\t=\\t\",k)\n",
    "    print(\"score\\t=\\t\",cv_results['test_score'].mean())\n",
    "    print(\"-------------------------------------------------\")\n",
    "    scores = np.append(scores, (k, cv_results['test_score'].mean()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54a0b214ce13796d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scores_df_2 = pd.DataFrame.from_records([(scores[i],scores[i+1]) for i in range(0,len(scores),2)],  columns=[\"k\", \"score\"])\n",
    "sns.lineplot(scores_df_2, x=\"k\", y=\"score\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1889499edacf916d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Der beste Score liegt bei 60."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e829a7f17309b8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "feature_selector = SelectKBest(chi2, k=1000).fit(X, y)\n",
    "X = X.iloc[:,feature_selector.get_support(indices=True)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58959f40ac6cb1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "decision_tree_cv = cross_validate(decision_tree, X_new, y, cv=5)\n",
    "print(\"Ergebnisse der einzelnen Folds: \",decision_tree_cv['test_score'])\n",
    "print(\"Durchschnittlicher Score: \",decision_tree_cv['test_score'].mean())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74768a68432f2b2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nun gilt es die besten Parameter zu finden, dazu werden zwei Werkzeuge benutzt: RandomizedSearchCV und GridSearchCV.\n",
    "Beide Methoden verwenden Kreuzvalidierung und generieren Scores für Modelle mit verschiedenen Parametern."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ac96d3d5bff2d8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b43430b1cb58860a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### RandomizedSearchCV\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "975b8b465cd49359"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    \"criterion\": ['entropy', 'gini'],\n",
    "    \"max_depth\" : range(1,51,1),\n",
    "    \"max_leaf_nodes\": range(10,110,10),\n",
    "    \"min_samples_leaf\" : range(50, 1000,50),\n",
    "    \"min_samples_split\" : range(50,1000,50),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "random_search_cv = RandomizedSearchCV(\n",
    "decision_tree, \n",
    "    param_distributions=param_grid, \n",
    "    n_iter=50, \n",
    "    verbose=5\n",
    "    \n",
    ")\n",
    "\n",
    "random_search_cv = random_search_cv.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eeacb2c5aa60f717"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# random_search_results = pd.read_csv(\"data/random_search.csv\")\n",
    "random_search_results = pd.DataFrame(random_search_cv.cv_results_)\n",
    "\n",
    "params_column = random_search_results[\"params\"]\n",
    "params_df = params_column.apply(lambda x: pd.Series(eval(str(x))))\n",
    "random_search_results = pd.concat([random_search_results, params_df], axis=1)\n",
    "random_search_results = random_search_results.drop([\"params\"], axis=1)\n",
    "\n",
    "random_search_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "facc36e2a8a80912"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rs_top_10 = random_search_results.loc[random_search_results[\"rank_test_score\"] <=10]\n",
    "rs_top_10 = rs_top_10.filter(like=\"param_\").iloc[:,0:4].astype('int64').describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e21bab387e67557b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rs_top_10"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80449fe5e2d90e91"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "random_search_cv.best_params_"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea37a3b030965a59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wenn man die Top 10 Parameter Konfigurationen betrachtet, lassen sich die Parameter weiter eingrenzen."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c583846c9f413a3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"criterion\": ['gini'],\n",
    "    \"min_samples_split\" : range(int(rs_top_10.loc[\"min\",\"param_min_samples_split\"]),int(rs_top_10.loc[\"max\",\"param_min_samples_split\"]),200),\n",
    "    \"min_samples_leaf\" : range(int(rs_top_10.loc[\"min\",\"param_min_samples_leaf\"]),int(rs_top_10.loc[\"max\",\"param_min_samples_leaf\"]),100),\n",
    "    \"max_leaf_nodes\": range(int(rs_top_10.loc[\"min\",\"param_max_leaf_nodes\"]),int(rs_top_10.loc[\"max\",\"param_max_leaf_nodes\"]),20),\n",
    "    \"max_depth\" : range(int(rs_top_10.loc[\"min\",\"param_max_depth\"]),int(rs_top_10.loc[\"max\",\"param_max_depth\"]),10)\n",
    "}\n",
    "\n",
    "print(param_grid)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "545c7ec81711751c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GridSearchCV"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b14c8f24538a335"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from joblib import parallel_backend\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-1):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    grid_search_cv = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_grid, cv= 5,verbose=5, n_jobs=-1)\n",
    "    grid_search_cv.fit(X, y)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3ceae14216c08f6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# grid_search_results = pd.read_csv(\"data/grid_search.csv\")\n",
    "grid_search_results = pd.DataFrame(grid_search_cv.cv_results_)\n",
    "\n",
    "params_column = grid_search_results[\"params\"]\n",
    "params_df = params_column.apply(lambda x: pd.Series(eval(str(x))))\n",
    "grid_search_results = pd.concat([grid_search_results, params_df], axis=1)\n",
    "grid_search_results = grid_search_results.drop([\"params\"], axis=1)\n",
    "\n",
    "grid_search_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15d0b13ef1ebd00b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "grid_search_results.loc[grid_search_results[\"rank_test_score\"] ==1].filter(like=\"param_\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "360d120eb455bd9b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(grid_search_results.best_params_,grid_search_results.best_score_)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d8ab0879138a776"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "random_search_results.to_csv(\"random_search_tree.csv\")\n",
    "grid_search_results.to_csv(\"grid_search_tree.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6c4a584266de0ab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimierter Baum"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7101d56b54ed3a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nun kann der finale Baum mit den Parametern generiert werden."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2386089c5277620"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(\n",
    "    criterion         = 'entropy',\n",
    "    max_depth         = 34,\n",
    "    max_leaf_nodes    = 100,\n",
    "    min_samples_leaf  = 500,\n",
    "    min_samples_split = 450\n",
    ")\n",
    "\n",
    "cv_tree = cross_validate(decision_tree, X, y, cv=5)\n",
    "cv_tree[\"test_score\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e24ccae37d54ae40"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = cross_val_predict(decision_tree, X, y, cv=5)\n",
    "\n",
    "conf_matrix = confusion_matrix(y, y_pred, labels=categories)\n",
    "\n",
    "#Konfusionsmatrix anzeigen (optional mit Seaborn für eine bessere Darstellung)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix with k-Fold Cross-Validation for Decision Tree')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b26ce0c30c392d40"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d0b518fc0264b676"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Support Vector Machines"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6bda67809b4f2a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Daten Filtern"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30e6ae2e2d703cff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Es gilt herauszufinden, welche Anzahl an Spalten die optimalste ist."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1a43b47518e1f3b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "scores = []\n",
    "k=2\n",
    "while k < X.shape[1]/2:\n",
    "    feature_selector = SelectKBest(chi2, k=k).fit(X, y)\n",
    "    X_temp = X.iloc[:,feature_selector.get_support(indices=True)]\n",
    "    cv_results = cross_validate(svm, X, y, cv=5)\n",
    "    print(\"k\\t\\t=\\t\",k)\n",
    "    print(\"score\\t=\\t\",cv_results['test_score'].mean())\n",
    "    print(\"-------------------------------------------------\")\n",
    "    scores = np.append(scores, (k, cv_results['test_score'].mean()))\n",
    "    k=k*2\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caf8940474462a04"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "59ac8a7b624e8711"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame.from_records([(scores[i],scores[i+1]) for i in range(0,len(scores),2)],  columns=[\"k\", \"score\"])\n",
    "sns.lineplot(scores_df, x=\"k\", y=\"score\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdfca751b6f2dc20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ein Entscheidungsbaum mit Standardeinstellungen lieferten die besten Werte bei einer Spaltenanzahl zwischen 32 und 128.\n",
    "\n",
    "Nun kann das Prozedere wiederholt werden mit kleineren Schritten."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7df20b3aea8cd9b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eee69cec5e003c70"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "svm = LinearSVC()\n",
    "\n",
    "scores = []\n",
    "for k in range(30,130,10):\n",
    "    feature_selector = SelectKBest(chi2, k=k).fit(X, y)\n",
    "    X_temp = X.iloc[:,feature_selector.get_support(indices=True)]\n",
    "    cv_results = cross_validate(svm, X_temp, y, cv=5)\n",
    "    print(\"k\\t\\t=\\t\",k)\n",
    "    print(\"score\\t=\\t\",cv_results['test_score'].mean())\n",
    "    print(\"-------------------------------------------------\")\n",
    "    scores = np.append(scores, (k, cv_results['test_score'].mean()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d995d680e7bad0ff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scores_df_2 = pd.DataFrame.from_records([(scores[i],scores[i+1]) for i in range(0,len(scores),2)],  columns=[\"k\", \"score\"])\n",
    "sns.lineplot(scores_df_2, x=\"k\", y=\"score\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ddee3f7882cc5de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Der beste Score liegt bei 60."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c8cc4aa6310397d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "feature_selector = SelectKBest(chi2, k=1000).fit(X, y)\n",
    "X = X.iloc[:,feature_selector.get_support(indices=True)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deadc0a137706d1e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Mit Einstellungen\n",
    "#0.5579648513453019\n",
    "\n",
    "#Ohne Einstellungen\n",
    "#0.5663729942716096\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "decision_tree_cv = cross_validate(decision_tree, X_new, y, cv=5)\n",
    "print(\"Ergebnisse der einzelnen Folds: \",decision_tree_cv['test_score'])\n",
    "print(\"Durchschnittlicher Score: \",decision_tree_cv['test_score'].mean())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aae67e04cf94df0a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nun gilt es die besten Parameter zu finden, dazu werden zwei Werkzeuge benutzt: RandomizedSearchCV und GridSearchCV.\n",
    "Beide Methoden verwenden Kreuzvalidierung und generieren Scores für Modelle mit verschiedenen Parametern."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "171349a140037756"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d4e08d08ef0722"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GridSearchCV"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "563e0ee8450b91e0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from joblib import parallel_backend\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'dual': [True, False],\n",
    "    'tol': [1e-3, 1e-4, 1e-5]\n",
    "}\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-1):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    grid_search_cv = GridSearchCV(estimator=svm, param_grid=param_grid, cv= 5,verbose=5, n_jobs=-1)\n",
    "    grid_search_cv.fit(X, y)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7b0f33f4661e8d6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# grid_search_results = pd.read_csv(\"data/grid_search.csv\")\n",
    "grid_search_results = pd.DataFrame(grid_search_cv.cv_results_)\n",
    "\n",
    "params_column = grid_search_results[\"params\"]\n",
    "params_df = params_column.apply(lambda x: pd.Series(eval(str(x))))\n",
    "grid_search_results = pd.concat([grid_search_results, params_df], axis=1)\n",
    "grid_search_results = grid_search_results.drop([\"params\"], axis=1)\n",
    "\n",
    "grid_search_results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cd10d9afe1ffa6c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "grid_search_results.loc[grid_search_results[\"rank_test_score\"] ==1].filter(like=\"param_\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c879240460d1fedb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(grid_search_results.best_params_,grid_search_results.best_score_)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed976de75ad32401"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "grid_search_results.to_csv(\"grid_search_svm.csv\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d68167fc020a6c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimierte Support Vector Machine"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e580f8d50332a91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nun kann das finale Modell mit den Parametern generiert werden."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c8e67e94d876a97"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "svm = LinearSVC(\n",
    "    C = 0.1,\n",
    "    loss = 'hinge',\n",
    "    penalty = 'l2',\n",
    "    dual = True,\n",
    "    tol = 1e-4\n",
    ")\n",
    "\n",
    "cross_validate(svm, X, y, cv=5)[\"test_score\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bda2d6586b3064ac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = cross_val_predict(svm, X, y, cv=5)\n",
    "\n",
    "conf_matrix = confusion_matrix(y, y_pred, labels=categories)\n",
    "\n",
    "#Konfusionsmatrix anzeigen (optional mit Seaborn für eine bessere Darstellung)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix with k-Fold Cross-Validation for Decision Tree')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cde8f4c5f8afd6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
